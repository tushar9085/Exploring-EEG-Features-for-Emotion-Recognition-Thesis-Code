{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy import signal\n",
    "import numpy as np\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(675, 4, 32, 42)\n",
      "(675,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def compute_DE(signal):\n",
    "    variance = np.var(signal, ddof=1)\n",
    "    return math.log(2 * math.pi * math.e * variance) / 2\n",
    "\n",
    "def eliminate_channels(data):\n",
    "    # Indices of rows to keep\n",
    "    indices_to_keep = [1, 4, 6, 8, 18, 16, 24, 26, 36, 34, 42, 44, 46, 53, 59, 60, 61, 55, 48, 50, 40, 38, 30, 32, 22, 20, 12, 14, 5, 3, 10, 28]\n",
    "\n",
    "    # Create a new array to store the selected rows\n",
    "    new_data = []\n",
    "\n",
    "    # Append selected rows to new_data\n",
    "    for i in range(len(indices_to_keep)):\n",
    "        new_data.append(data[indices_to_keep[i]-1])\n",
    "\n",
    "    # Convert new_data to a NumPy array\n",
    "    return np.array(new_data)\n",
    "\n",
    "def load_data():\n",
    "    data_dir = \"E:/STUDY/Publications/Thesis/Brain Emotion Detection/Dataset/SEED/SEED_EEG/Preprocessed_EEG/\"\n",
    "    fs = 200\n",
    "    fStart = [4, 8, 14, 31]\n",
    "    fEnd = [7, 13, 30, 50]\n",
    "    all_channels = range(62)\n",
    "    selected_channels = range(32)\n",
    "    seconds_to_exclude = 50\n",
    "\n",
    "    filename_label = \"label\"\n",
    "    label = loadmat(data_dir + filename_label)\n",
    "    label = label[\"label\"][0]\n",
    "\n",
    "    datasets_X, datasets_y = [], []\n",
    "\n",
    "    for filename_data in os.listdir(data_dir):\n",
    "        if filename_data in [\"label.mat\", \"readme.txt\"]:\n",
    "            continue\n",
    "\n",
    "        data_all = loadmat(data_dir + filename_data)\n",
    "        scenes = list(data_all.keys())[3:]\n",
    "\n",
    "        for index, scene in enumerate(scenes):\n",
    "            dataset_X = []\n",
    "            data = data_all[scene][:, :37000]  # Considering all channels\n",
    "            \n",
    "            new_data = eliminate_channels(data)\n",
    "            data = np.array(new_data)\n",
    "\n",
    "            start_index = seconds_to_exclude * fs\n",
    "            \n",
    "\n",
    "            data = data[:, start_index:]\n",
    "\n",
    "            for band_index, band in enumerate(fStart):\n",
    "                b, a = signal.butter(4, [fStart[band_index]/fs, fEnd[band_index]/fs], 'bandpass')\n",
    "                filtedData = signal.filtfilt(b, a, data)\n",
    "                filtedData_de = []\n",
    "\n",
    "                for lead in selected_channels:\n",
    "                    filtedData_split = []\n",
    "\n",
    "                    for de_index in range(0, filtedData.shape[1] - int(fs*3.2), int(fs*3.2)):\n",
    "                        filtedData_split.append(compute_DE(filtedData[lead, de_index: de_index + int(fs*3.2)]))\n",
    "\n",
    "                    filtedData_de.append(filtedData_split)\n",
    "                \n",
    "                filtedData_de = np.array(filtedData_de)\n",
    "                dataset_X.append(filtedData_de)\n",
    "\n",
    "            datasets_X.append(dataset_X)\n",
    "            datasets_y.append(label[index])\n",
    "\n",
    "    datasets_X, datasets_y = np.array(datasets_X), np.array(datasets_y)\n",
    "\n",
    "    return datasets_X, datasets_y\n",
    "\n",
    "\n",
    "datasets_X, datasets_y = load_data()\n",
    "print(datasets_X.shape)\n",
    "print(datasets_y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(675, 32, 42, 4)\n"
     ]
    }
   ],
   "source": [
    "new_datasets_X = np.copy(datasets_X)\n",
    "new_datasets_X = np.transpose(new_datasets_X, (0, 2, 3, 1))\n",
    "print(new_datasets_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('SEED_DE_X_for_Merge_2min.npy', new_datasets_X)\n",
    "np.save('SEED_DE_y_for_Merge_2min.npy', datasets_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_code_practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
